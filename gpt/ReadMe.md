# Very small GPT model 
In this model pretraining of the GPT model is done using decoder only architecture . Initially bigram model is used to train the model according to the dataset where negative loglikelihood is used as the loss function .

After we implement attention , masked attention mechanism from the scratch and implemented decoder only architecture to generate the text according to the dataset

```All the explanation of the code is done in the notebook which can be easily understood```

# Installation
1. To run in local 
   1. `` pip install torch``
   2. ``jupyter notebook gpt.ipynb``

